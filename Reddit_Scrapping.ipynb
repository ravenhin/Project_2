{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting a certain subreddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook looks at two subreddits of two political candidates, Bernie Sanders and Joe Biden. By comparing the titles of the posts of each subreddit, I created a model that would predict which title came from a certain subreddit. Using the API from the reddit website I was able to pull the data I needed from the subreddits to create a model from the titles on the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using the requests libaray, I pulled the data from the subreddit websites. Below I have pulled the data from the\n",
    "#Bernie Sanders subreddit website on Reddit.\n",
    "\n",
    "import requests\n",
    "\n",
    "bern_url = 'https://api.pushshift.io/reddit/search/submission?subreddit=SandersForPresident&size=1000'\n",
    "\n",
    "res_1 = requests.get(bern_url)\n",
    "\n",
    "res_1.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The API link brings the website in as a json file, so below I coverted the json file as a dictionary.\n",
    "data = res_1.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'all_awardings': [],\n",
       " 'allow_live_comments': False,\n",
       " 'author': 'fenspyre',\n",
       " 'author_flair_css_class': None,\n",
       " 'author_flair_richtext': [],\n",
       " 'author_flair_text': None,\n",
       " 'author_flair_type': 'text',\n",
       " 'author_fullname': 't2_is6bz',\n",
       " 'author_patreon_flair': False,\n",
       " 'author_premium': False,\n",
       " 'awarders': [],\n",
       " 'can_mod_post': False,\n",
       " 'contest_mode': False,\n",
       " 'created_utc': 1580320506,\n",
       " 'domain': 'self.SandersForPresident',\n",
       " 'full_link': 'https://www.reddit.com/r/SandersForPresident/comments/evqd4x/my_fundraising_pledge/',\n",
       " 'gildings': {},\n",
       " 'id': 'evqd4x',\n",
       " 'is_crosspostable': True,\n",
       " 'is_meta': False,\n",
       " 'is_original_content': False,\n",
       " 'is_reddit_media_domain': False,\n",
       " 'is_robot_indexable': True,\n",
       " 'is_self': True,\n",
       " 'is_video': False,\n",
       " 'link_flair_background_color': '',\n",
       " 'link_flair_richtext': [],\n",
       " 'link_flair_text_color': 'dark',\n",
       " 'link_flair_type': 'text',\n",
       " 'locked': False,\n",
       " 'media_only': False,\n",
       " 'no_follow': True,\n",
       " 'num_comments': 0,\n",
       " 'num_crossposts': 0,\n",
       " 'over_18': False,\n",
       " 'parent_whitelist_status': 'all_ads',\n",
       " 'permalink': '/r/SandersForPresident/comments/evqd4x/my_fundraising_pledge/',\n",
       " 'pinned': False,\n",
       " 'pwls': 6,\n",
       " 'retrieved_on': 1580320513,\n",
       " 'score': 1,\n",
       " 'selftext': \"Today I realized I've contributed $500 toward the 2020 campaign so far.  I also recognized that I really need to see this through, and if I've made that much of a sacrifice for what I want so far, I'll do it again.  \\n\\n\\nIf Bernie wins either NH or IA, I'm going to double my donation thus far.\\n\\nIf Bernie wins the nomination... I'm doubling it again.  \\n\\n\\nThat's $2000 toward the campaign already, which gives me a little room to save some donation room for those key points that will come up during the rest of the race.  \\n\\n&amp;#x200B;\\n\\nThis is my pledge, and I just wanted to share it with you.  Thank you all who do all of the work on the ground that I'm not able to.  This is the most I can do for you right now, but if things go well for me and my future, I'll have some time on the ground too!\\n\\n&amp;#x200B;\\n\\nMuch love to all of you heroes out there.  It's all of US.\",\n",
       " 'send_replies': True,\n",
       " 'spoiler': False,\n",
       " 'stickied': False,\n",
       " 'subreddit': 'SandersForPresident',\n",
       " 'subreddit_id': 't5_2zbq7',\n",
       " 'subreddit_subscribers': 387624,\n",
       " 'subreddit_type': 'public',\n",
       " 'suggested_sort': 'confidence',\n",
       " 'thumbnail': 'self',\n",
       " 'title': 'My Fundraising Pledge',\n",
       " 'total_awards_received': 0,\n",
       " 'url': 'https://www.reddit.com/r/SandersForPresident/comments/evqd4x/my_fundraising_pledge/',\n",
       " 'whitelist_status': 'all_ads',\n",
       " 'wls': 6}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Below is the first dictionary of the list of dictionaries.\n",
    "data['data'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Data: Bernie Sanders Subreddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to explore the data from the subreddit, I needed to convert the list of dictionaries into a dataframe, and then I wanted to see what the top 2 word phrases were from the title's that were read. In order to see the two word phrases I needed to count vectorize the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raven\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\raven\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "bernie = pd.DataFrame(data['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bernie_df = bernie[['title', 'subreddit']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raven\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(2, 2), preprocessor=None, stop_words='english',\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bernie_words = CountVectorizer(stop_words ='english', ngram_range = (2,2))\n",
    "\n",
    "X = bernie['title']\n",
    "y = bernie['subreddit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n",
    "\n",
    "bernie_words.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bern_words = pd.DataFrame(bernie_words.transform(X_train).todense(), columns = bernie_words.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Top two-words phrases from Bernie Sanders subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bernie sanders           111\n",
       "joe biden                 20\n",
       "new hampshire             11\n",
       "bernie campaign           10\n",
       "tom perez                  8\n",
       "iowa caucus                8\n",
       "poll sanders               8\n",
       "days iowa                  8\n",
       "iowa poll                  8\n",
       "iowa caucuses              7\n",
       "poll biden                 7\n",
       "sanders campaign           7\n",
       "warren 15                  7\n",
       "ads bernie                 6\n",
       "democratic operatives      6\n",
       "attack ads                 6\n",
       "sanders iowa               6\n",
       "social security            6\n",
       "2008 vs                    6\n",
       "people perfume             6\n",
       "super pac                  6\n",
       "super tuesday              6\n",
       "support bernie             6\n",
       "family friends             6\n",
       "new deal                   5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bern_words.sum().sort_values(ascending = False).head(25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joe Biden subreddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I used the same work flow as reading in the Bernie Sanders subreddit to read in the Joe Biden subreddit and explored the data in the same way as I did for the Bernie Sanders data. I wanted to see what the top two word phrases of the Joe Biden data was in order to compare it to the Sanders subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "biden_url = 'https://api.pushshift.io/reddit/search/submission?subreddit=JoeBiden&size=1000'\n",
    "\n",
    "res_2 = requests.get(biden_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_2.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "biden_data = res_2.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'all_awardings': [],\n",
       " 'allow_live_comments': False,\n",
       " 'author': 'Fiery1Phoenix',\n",
       " 'author_flair_css_class': None,\n",
       " 'author_flair_richtext': [],\n",
       " 'author_flair_text': None,\n",
       " 'author_flair_type': 'text',\n",
       " 'author_fullname': 't2_udkhn',\n",
       " 'author_patreon_flair': False,\n",
       " 'author_premium': True,\n",
       " 'awarders': [],\n",
       " 'can_mod_post': False,\n",
       " 'contest_mode': False,\n",
       " 'created_utc': 1580320022,\n",
       " 'domain': 'projects.fivethirtyeight.com',\n",
       " 'full_link': 'https://www.reddit.com/r/JoeBiden/comments/evq8xb/biden_currently_has_a_52_chance_of_winning_a/',\n",
       " 'gildings': {},\n",
       " 'id': 'evq8xb',\n",
       " 'is_crosspostable': True,\n",
       " 'is_meta': False,\n",
       " 'is_original_content': False,\n",
       " 'is_reddit_media_domain': False,\n",
       " 'is_robot_indexable': True,\n",
       " 'is_self': False,\n",
       " 'is_video': False,\n",
       " 'link_flair_background_color': '',\n",
       " 'link_flair_richtext': [],\n",
       " 'link_flair_text_color': 'dark',\n",
       " 'link_flair_type': 'text',\n",
       " 'locked': False,\n",
       " 'media_only': False,\n",
       " 'no_follow': True,\n",
       " 'num_comments': 0,\n",
       " 'num_crossposts': 0,\n",
       " 'over_18': False,\n",
       " 'permalink': '/r/JoeBiden/comments/evq8xb/biden_currently_has_a_52_chance_of_winning_a/',\n",
       " 'pinned': False,\n",
       " 'retrieved_on': 1580320023,\n",
       " 'score': 1,\n",
       " 'selftext': '',\n",
       " 'send_replies': True,\n",
       " 'spoiler': False,\n",
       " 'stickied': False,\n",
       " 'subreddit': 'JoeBiden',\n",
       " 'subreddit_id': 't5_2t0th',\n",
       " 'subreddit_subscribers': 3104,\n",
       " 'subreddit_type': 'public',\n",
       " 'thumbnail': 'default',\n",
       " 'title': 'Biden currently has a 52% chance of winning a plurality of delegates!',\n",
       " 'total_awards_received': 0,\n",
       " 'url': 'https://projects.fivethirtyeight.com/2020-primary-forecast/'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biden_data['data'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Created a dataframe of Joe Biden subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "biden = pd.DataFrame(biden_data['data'])\n",
    "\n",
    "biden_df = biden[['title', 'subreddit']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(2, 2), preprocessor=None, stop_words='english',\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biden_words = CountVectorizer(stop_words ='english', ngram_range = (2,2))\n",
    "\n",
    "X = biden['title']\n",
    "y = biden['subreddit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n",
    "\n",
    "biden_words.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "biden_words = pd.DataFrame(biden_words.transform(X_train).todense(), columns = biden_words.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explored the top two phrases of Joe Biden data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joe biden                  251\n",
       "daily roundtable            34\n",
       "poll biden                  33\n",
       "endorses joe                26\n",
       "biden president             24\n",
       "roundtable january          24\n",
       "bernie sanders              22\n",
       "donald trump                18\n",
       "biden leads                 15\n",
       "biden twitter               15\n",
       "social security             15\n",
       "democratic presidential     14\n",
       "national poll               12\n",
       "south carolina              11\n",
       "foreign policy              11\n",
       "new hampshire               10\n",
       "2020 democratic             10\n",
       "warren 15                   10\n",
       "buttigieg bloomberg         10\n",
       "democratic primary          10\n",
       "roundtable december         10\n",
       "vice president              10\n",
       "biden holds                  9\n",
       "biden says                   9\n",
       "poll shows                   9\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biden_words.sum().sort_values(ascending = False).head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Bernie and Biden dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I combined the dataframes, from the Bernie and Biden subreddits, that I created earlier in order to make a model that will predict which titles come from which subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My Fundraising Pledge</td>\n",
       "      <td>SandersForPresident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Google, I think your bias is showing</td>\n",
       "      <td>SandersForPresident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New Bernie Sanders Campaign Ad shows how he's ...</td>\n",
       "      <td>SandersForPresident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ðŸ¥³ðŸ¥³ðŸ¥³ðŸ¥³</td>\n",
       "      <td>SandersForPresident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Texas Poll: Trump 50%, Sanders 47% (Better tha...</td>\n",
       "      <td>SandersForPresident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Joe Biden unveils new Texas endorsements as he...</td>\n",
       "      <td>JoeBiden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Is Biden unwell?</td>\n",
       "      <td>JoeBiden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Cesar Torres on Twitter: Had the pleasure to s...</td>\n",
       "      <td>JoeBiden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>538 just released their primary polling model</td>\n",
       "      <td>JoeBiden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Cedar Rapids teacher, former ISEA (Iowa State ...</td>\n",
       "      <td>JoeBiden</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title            subreddit\n",
       "0                                My Fundraising Pledge  SandersForPresident\n",
       "1                 Google, I think your bias is showing  SandersForPresident\n",
       "2    New Bernie Sanders Campaign Ad shows how he's ...  SandersForPresident\n",
       "3                                                 ðŸ¥³ðŸ¥³ðŸ¥³ðŸ¥³  SandersForPresident\n",
       "4    Texas Poll: Trump 50%, Sanders 47% (Better tha...  SandersForPresident\n",
       "..                                                 ...                  ...\n",
       "995  Joe Biden unveils new Texas endorsements as he...             JoeBiden\n",
       "996                                   Is Biden unwell?             JoeBiden\n",
       "997  Cesar Torres on Twitter: Had the pleasure to s...             JoeBiden\n",
       "998      538 just released their primary polling model             JoeBiden\n",
       "999  Cedar Rapids teacher, former ISEA (Iowa State ...             JoeBiden\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bern_bid = pd.concat([bernie_df, biden_df])\n",
    "\n",
    "bern_bid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here I needed change the y varaible into an interger in order to set it up for creating a model.\n",
    "\n",
    "bern_bid['subreddit'] = bern_bid['subreddit'].map({'SandersForPresident': 0, 'JoeBiden': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My Fundraising Pledge</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Google, I think your bias is showing</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New Bernie Sanders Campaign Ad shows how he's ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ðŸ¥³ðŸ¥³ðŸ¥³ðŸ¥³</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Texas Poll: Trump 50%, Sanders 47% (Better tha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Joe Biden unveils new Texas endorsements as he...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Is Biden unwell?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Cesar Torres on Twitter: Had the pleasure to s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>538 just released their primary polling model</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Cedar Rapids teacher, former ISEA (Iowa State ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  subreddit\n",
       "0                                My Fundraising Pledge          0\n",
       "1                 Google, I think your bias is showing          0\n",
       "2    New Bernie Sanders Campaign Ad shows how he's ...          0\n",
       "3                                                 ðŸ¥³ðŸ¥³ðŸ¥³ðŸ¥³          0\n",
       "4    Texas Poll: Trump 50%, Sanders 47% (Better tha...          0\n",
       "..                                                 ...        ...\n",
       "995  Joe Biden unveils new Texas endorsements as he...          1\n",
       "996                                   Is Biden unwell?          1\n",
       "997  Cesar Torres on Twitter: Had the pleasure to s...          1\n",
       "998      538 just released their primary polling model          1\n",
       "999  Cedar Rapids teacher, former ISEA (Iowa State ...          1\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bern_bid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title        0\n",
       "subreddit    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bern_bid.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a Logistic Regression, KNeighbors, and Multinomial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bern_bid['title']\n",
    "y = bern_bid['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Created a pipeline for Logistic Regression and parameters for the models in order to find the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_bb = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('logr', LogisticRegressionCV(solver = 'liblinear', cv = 5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params_bb = {\n",
    "    'cvec__max_features': [100, 200, 300, 400, 500],\n",
    "    'cvec__max_df': [.98, .95],\n",
    "    'cvec__min_df': [2, 3],\n",
    "    'cvec__ngram_range': [(1,1)],\n",
    "    'cvec__stop_words': ['english']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Below is the GridSearch for the Logistic Regression, and scores for the train and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n",
      "0.876\n"
     ]
    }
   ],
   "source": [
    "gs_bb = GridSearchCV(pipe_bb, pipe_params_bb, cv = 5)\n",
    "\n",
    "gs_bb.fit(X_train, y_train)\n",
    "\n",
    "print(gs_bb.score(X_train, y_train))\n",
    "print(gs_bb.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.98,\n",
       " 'cvec__max_features': 400,\n",
       " 'cvec__min_df': 2,\n",
       " 'cvec__ngram_range': (1, 1),\n",
       " 'cvec__stop_words': 'english'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_bb.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Below is the KNeighbors model along with the scores for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_bb_knn = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_knn = GridSearchCV(pipe_bb_knn, pipe_params_bb, cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_knn.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.98,\n",
       " 'cvec__max_features': 200,\n",
       " 'cvec__min_df': 2,\n",
       " 'cvec__ngram_range': (1, 1),\n",
       " 'cvec__stop_words': 'english'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_knn.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8486666666666667"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_knn.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.856"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Below is the pipeline for the Mutlinomial model along with the scores for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_bb_mnb = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "gs_mnb = GridSearchCV(pipe_bb_mnb, pipe_params_bb, cv = 5)\n",
    "\n",
    "gs_mnb.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.98,\n",
       " 'cvec__max_features': 500,\n",
       " 'cvec__min_df': 3,\n",
       " 'cvec__ngram_range': (1, 1),\n",
       " 'cvec__stop_words': 'english'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_mnb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8273333333333334"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_mnb.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_mnb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Below is the pipeline for the Random Forest along with the scores for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_bb_rf = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('rf', RandomForestClassifier(n_estimators = 100))\n",
    "])\n",
    "\n",
    "gs_rf = GridSearchCV(pipe_bb_rf, pipe_params_bb, cv = 5)\n",
    "\n",
    "gs_rf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9793333333333333"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_rf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.95,\n",
       " 'cvec__max_features': 500,\n",
       " 'cvec__min_df': 3,\n",
       " 'cvec__ngram_range': (1, 1),\n",
       " 'cvec__stop_words': 'english'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_rf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
